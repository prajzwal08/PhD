{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d9cb34-a36b-4f36-a8f3-b986de9f6288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy import interpolate\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c46012-39ad-49be-8c1e-23db71711522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders_with_prefix(location, prefix):\n",
    "    \"\"\"\n",
    "    Retrieves a list of folder names within the specified location directory that start with the provided prefix.\n",
    "    \n",
    "    Parameters:\n",
    "        location (str): The directory path where the function will search for folders.\n",
    "        prefix (str): The prefix that the desired folders should start with.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of folder names starting with the specified prefix within the given location.\n",
    "    \"\"\"\n",
    "    folders_with_prefix = [folder for folder in os.listdir(location) if os.path.isdir(os.path.join(location, folder)) and folder.startswith(prefix)]\n",
    "    return folders_with_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d33a95-9143-4e3b-b4a9-69d656fc4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_csv_files_in_folder(folder_path, keyword):\n",
    "    \"\"\"\n",
    "    Retrieves a list of file paths for CSV files within the specified folder_path directory that contain the provided keyword in their filenames.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str): The directory path where the function will search for CSV files.\n",
    "        keyword (str): The keyword that the filenames of desired CSV files should contain.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of file paths for CSV files containing the specified keyword within the given folder_path.\n",
    "    \"\"\"\n",
    "    csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv') and keyword in file]\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d9ad13a-89ff-4d9d-8361-5366a6298c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negative_with_mean_of_nearest(arr):\n",
    "    \"\"\"\n",
    "    Replaces negative values in the input array with the mean of the nearest non-negative values.\n",
    "    \n",
    "    Parameters:\n",
    "        arr (numpy.ndarray): Input array.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array with negative values replaced by the mean of the nearest non-negative values.\n",
    "    \"\"\"\n",
    "    neg_indices = np.where(arr < 0)[0]  # Get indices where values are less than zero\n",
    "    for i in neg_indices:\n",
    "        # Find nearest non-negative values before and after the negative value\n",
    "        left_index = i - 1\n",
    "        while left_index in neg_indices:\n",
    "            left_index -= 1\n",
    "        right_index = i + 1\n",
    "        while right_index in neg_indices:\n",
    "            right_index += 1\n",
    "    \n",
    "        # Replace negative value with the mean of the nearest non-negative values\n",
    "        arr[i] = np.mean([arr[left_index], arr[right_index]])\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566b458b-42d1-4786-ac89-a2db99aa5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cams_co2_data(longitude, latitude, start_time, end_time, file_path):\n",
    "    \"\"\"\n",
    "    Retrieve CAMS CO2 data for a specific location and time period.\n",
    "    \n",
    "    Parameters:\n",
    "    longitude (float): Longitude of the location.\n",
    "    latitude (float): Latitude of the location.\n",
    "    start_time (str): Start time in the format 'YYYY-MM-DD'.\n",
    "    end_time (str): End time in the format 'YYYY-MM-DD'.\n",
    "    file_path (str): Path to the CAMS netCDF file.\n",
    "    \n",
    "    Returns:\n",
    "    co2_data (np.ndarray): Array of CO2 data.\n",
    "    \"\"\"\n",
    "     # Constants\n",
    "    M_CO2 = 44.01  # Molar mass of CO2 in g/mol\n",
    "    M_dry_air = 28.97  # Molar mass of dry air in g/mol\n",
    "\n",
    "     # Conversion factor from kg/kg to ppm\n",
    "    conversion_factor = 1e6  # ppm\n",
    "    \n",
    "    # Open the CAMS dataset\n",
    "    cams = xr.open_dataset(file_path).sortby('time')\n",
    "    \n",
    "    # Select the nearest location to the provided longitude and latitude\n",
    "    cams_location_selected = cams.sel(latitude=latitude, longitude=longitude, method='nearest')\n",
    "    \n",
    "    # Select the time range\n",
    "    cams_date_selected = cams_location_selected.sel(time=slice(start_time, end_time))\n",
    "    \n",
    "    # Extract time and CO2 variables\n",
    "    time_data = cams_date_selected['time'].values\n",
    "    co2_data = cams_date_selected['co2'].values.reshape(-1)\n",
    "    \n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame({'time': time_data, 'co2': co2_data})\n",
    "    \n",
    "    # Set 'time' column as index\n",
    "    df.set_index('time', inplace=True)\n",
    "    \n",
    "    # Resample to 30-minute intervals and forward fill missing values\n",
    "    df_filled = df.resample('30min').ffill()\n",
    "    \n",
    "    # Extend the index until 'end_time' and forward fill\n",
    "    end_date_extend = pd.to_datetime(end_time)\n",
    "    df_filled_new = df_filled.reindex(pd.date_range(start=df_filled.index.min(), end=end_date_extend, freq='30min')).ffill()\n",
    "    \n",
    "    # Extract CO2 data as numpy array\n",
    "    co2 = np.array(df_filled_new['co2'])\n",
    "    \n",
    "    # Convert kg/kg to ppm\n",
    "    co2_ppm = co2 * (conversion_factor * M_dry_air / M_CO2)\n",
    "    \n",
    "    return co2_ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8653ed32-aace-47b2-a7f5-6ed60ea45df0",
   "metadata": {},
   "outputs": [],
   "source": [
    " def calculate_RH_and_q(vpd, Tair, pressure):\n",
    "    \"\"\"\n",
    "    Calculate relative humidity (RH) and specific humidity (q) from input vpd, Tair, and pressure.\n",
    "    \n",
    "    Parameters:\n",
    "    vpd (array-like): Array of vapor pressure deficit values.\n",
    "    Tair (array-like): Array of air temperature values (in Celsius).\n",
    "    pressure (array-like): Array of air pressure values (in KPa).\n",
    "    \n",
    "    Returns:\n",
    "    RH (array-like): Array of relative humidity values.\n",
    "    q (array-like): Array of specific humidity values.\n",
    "    \"\"\"\n",
    "    # Calculating saturation vapor pressure (es) from the Tair.\n",
    "    es = 0.6108 * np.exp((17.27 * Tair) / (Tair + 237.3)) * 10\n",
    "    \n",
    "    # Calculating the actual vapor pressure from es and vpd and then RH.\n",
    "    ea = es - vpd\n",
    "    RH = (ea / es) * 100\n",
    "    RH_modified = replace_negative_with_mean_of_nearest(RH)\n",
    "    \n",
    "    \n",
    "    # Gas constant of dry air and vapor.\n",
    "    Rd = 287.058\n",
    "    Rv = 461.5\n",
    "    \n",
    "    # Calculating the specific humidity (q) from ea.\n",
    "    w = ea * Rd / (Rv * (pressure * 10 - ea))  # Since pressure in KPa, ea in hPa, same as VPD.\n",
    "    q = w / (w + 1)\n",
    "    qair_modified = replace_negative_with_mean_of_nearest(q)\n",
    "    \n",
    "    return RH_modified, qair_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ef19ac-9e2c-4066-a5ff-abd51a3396a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_weighted_LAI(lai_pixel, sd_pixel, qc_pixel):\n",
    "    \"\"\"\n",
    "    Calculates the spatially weighted Leaf Area Index (LAI) based on the input LAI values, standard deviation (SD) values, and quality control (QC) flags.\n",
    "    \n",
    "    Parameters:\n",
    "        lai_pixel (numpy.ndarray): Array containing the LAI values for each pixel.\n",
    "        sd_pixel (numpy.ndarray): Array containing the standard deviation values for each pixel.\n",
    "        qc_pixel (numpy.ndarray): Array containing the quality control flags for each pixel.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array containing the spatially weighted LAI values.\n",
    "    \"\"\"\n",
    "    # Use only good quality data\n",
    "    \n",
    "    qc_flags = [0, 2, 24, 26, 32, 34, 56, 58]\n",
    "    \n",
    "    # Mask the quality flag\n",
    "    mask = np.isin(qc_pixel, qc_flags)\n",
    "    lai_pixel[~mask] = np.nan\n",
    "    sd_pixel[~mask] = np.nan\n",
    "    \n",
    "    # Mask out where sd is really low (likely cloud effects)\n",
    "    sd_pixel[sd_pixel < 0.1] = np.nan\n",
    "    lai_pixel[np.isnan(sd_pixel)] = np.nan\n",
    "    \n",
    "    # Set the values above threshold to missing\n",
    "    lai_pixel[lai_pixel > 10] = np.nan\n",
    "    \n",
    "    # Calculate weights, ignoring NaN values\n",
    "    weights = (1 / sd_pixel**2) / np.nansum(1 / sd_pixel**2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Element-wise multiplication of lai_pixel and weights\n",
    "    weighted_lai_values = lai_pixel * weights\n",
    "    \n",
    "    # Calculate the weighted mean for each row, ignoring NaN values\n",
    "    weighted_lai = np.nanmean(weighted_lai_values, axis=1)\n",
    "    \n",
    "    return weighted_lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb16a66-cc1c-49d7-a22f-94eb11d21bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_NA_LAI(unfilled_lai):\n",
    "    \"\"\"\n",
    "    Interpolates missing values (NaNs) in a given array of LAI (Leaf Area Index) using cubic interpolation and caps negative values to zero.\n",
    "    \n",
    "    Parameters:\n",
    "        unfilled_lai (numpy.ndarray): Array containing LAI values with missing values represented as NaNs.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array with missing values filled using interpolation and negative values capped at zero.\n",
    "    \"\"\"\n",
    "    filled_lai = unfilled_lai.copy()\n",
    "    \n",
    "    # Create a mask for NaN values\n",
    "    nan_mask = np.isnan(unfilled_lai)\n",
    "    \n",
    "    # Generate an index array for values\n",
    "    x = np.arange(len(unfilled_lai))\n",
    "    \n",
    "    # Interpolate only at the positions where NaNs are present\n",
    "    interp_func = interpolate.interp1d(x[~nan_mask], unfilled_lai[~nan_mask], kind='cubic', fill_value=\"extrapolate\")\n",
    "    \n",
    "    # Extrapolate the NaN values\n",
    "    filled_lai[nan_mask] = interp_func(x)[nan_mask]\n",
    "    \n",
    "    # Cap negative values to zero\n",
    "    filled_lai[filled_lai < 0] = 0\n",
    "    \n",
    "    # Set the last observation to zero\n",
    "    filled_lai[-1] = 0\n",
    "    \n",
    "    return filled_lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2cb1704-d4b4-4bff-9de2-2a34d18e5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_availability_LAI(filled_lai,lai_time):\n",
    "    \"\"\"\n",
    "    Checks the availability of MODIS LAI data and fills missing values if necessary. It ensures that the provided time range matches the expected range for MODIS observations.\n",
    "    \n",
    "    Parameters:\n",
    "        filled_lai (numpy.ndarray): Array containing MODIS LAI data.\n",
    "        lai_time (pandas.Series): Series containing timestamps corresponding to the MODIS LAI data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the filled LAI data and the corresponding dates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if filled_lai has no NAs or not, because sometimes MODIS observations are missing in between.\n",
    "    all_tsteps = []\n",
    "    \n",
    "    # Loop over each year from start to end of MODIS data \n",
    "    for year in range(lai_time.dt.year.min(), lai_time.dt.year.max()+1):\n",
    "        # Generate the first two dates for the year\n",
    "        #year_dates = pd.date_range(start=f\"{year}-01-01\", periods=2, freq='8D')\n",
    "        \n",
    "        # Ensure 46 evenly spaced dates for the rest of the year\n",
    "        year_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\", freq='8D')\n",
    "        \n",
    "        # Append the datetime series for the current year to the list\n",
    "        all_tsteps.extend(year_dates)\n",
    "    \n",
    "    #Change to pd datetime. \n",
    "    all_time = pd.to_datetime(all_tsteps)\n",
    "    #Check if all date spaced 8 days apart are present in original MODIS lai_time. \n",
    "    result = all_time.isin(lai_time)\n",
    "    \n",
    "    temp_array = np.full(result.shape, np.nan)\n",
    "    \n",
    "    # Fill new array based on the condition of result.\n",
    "    #This basically fills the point after checking date. \n",
    "    fill_index = 0\n",
    "    for i, val in enumerate(result):\n",
    "        if val:\n",
    "            temp_array[i] = filled_lai[fill_index]\n",
    "            fill_index += 1\n",
    "    \n",
    "    selected_lai = temp_array.reshape(-1, 46)[1:-1] #Basically because each year MODIS has 46 observations, if all are available.\n",
    "    # Select dates from 2003 to 2022\n",
    "    selected_dates = all_time[(all_time.year >= 2003) & (all_time.year <= 2023)]\n",
    "    \n",
    "    if len(np.isnan(selected_lai).flatten()) > 0:\n",
    "        selected_lai_flatten = selected_lai.reshape(-1)\n",
    "        positions = np.where(np.isnan(selected_lai_flatten))[0]\n",
    "    \n",
    "        if len(positions) > 0:\n",
    "            for position in positions:\n",
    "                selected_lai_flatten[position] = (selected_lai_flatten[position-1]+selected_lai_flatten[position+1])/2 #filling with the average of two nearest value\n",
    "            gap_free_lai = selected_lai_flatten.reshape(-1, 46)\n",
    "            return gap_free_lai,selected_dates\n",
    "    else:\n",
    "        return selected_lai,selected_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35a2f96-f578-4eb5-abc7-57a969510c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate rolling mean along rows\n",
    "def rolling_mean(array, window):\n",
    "    \"\"\"\n",
    "    Calculates the rolling mean along a 1D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "        array (numpy.ndarray): Input 1D array.\n",
    "        window (int): Size of the rolling window.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Resultant array after applying the rolling mean.\n",
    "    \"\"\"\n",
    "    result = np.full_like(array, np.nan)\n",
    "    for i in range(array.shape[0]):\n",
    "        start_index = max(0, i - window // 2)\n",
    "        end_index = min(array.shape[0], i + (window + 1) // 2)\n",
    "        result[i] = np.mean(array[start_index:end_index], axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20288c35-321b-40fa-81b4-c37eb457aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_LAI(gap_free_lai):\n",
    "    \"\"\"\n",
    "    Smoothes the gap-free LAI data by calculating climatology, removing mean climatology to obtain anomalies,\n",
    "    and applying a rolling mean to the anomalies with a window of +/- 6 months. Finally, it adds the smoothed anomalies\n",
    "    back to the climatology to obtain smoothed LAI values.\n",
    "    \n",
    "    Parameters:\n",
    "        gap_free_lai (numpy.ndarray): Array containing gap-free LAI data.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Smoothed LAI values.\n",
    "    \"\"\"\n",
    "    # Calculate the mean of each column ignoring NaNs (climatology)\n",
    "    column_means = np.nanmean(gap_free_lai, axis=0)\n",
    "    smoothed_column_means = savgol_filter(column_means, window_length = 13, polyorder = 3)\n",
    "    # Calculate the anomaly after removing mean climatology. \n",
    "    anomaly = gap_free_lai - column_means\n",
    "    # Calculate running mean anomaly (+/- 6 months either side of each time step)\n",
    "    anomaly_rolling = rolling_mean(anomaly.flatten(), 13)\n",
    "    smoothed_lai = anomaly_rolling + np.tile(smoothed_column_means,(anomaly.shape[0]))\n",
    "    return smoothed_lai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a19e3c2-b9db-4aa0-8cb2-cd845118c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LAI(modis_path,station_name,start_date,end_date):\n",
    "    \"\"\"\n",
    "    Retrieves Leaf Area Index (LAI) data from MODIS files for a specific station and time range. \n",
    "    The function performs data preprocessing steps including spatial weighting, interpolating missing values, \n",
    "    checking data availability, smoothing LAI, and resampling to match the resolution of the flux tower data.\n",
    "    \n",
    "    Parameters:\n",
    "        modis_path (str): Path to the directory containing MODIS files.\n",
    "        station_name (str): Name of the station.\n",
    "        start_date (str): Start date of the desired time range (format: 'YYYY-MM-DD').\n",
    "        end_date (str): End date of the desired time range (format: 'YYYY-MM-DD').\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array containing the smoothed LAI values.\n",
    "    \"\"\"\n",
    "    \n",
    "    lai_file = glob.glob(f\"{modis_path}/{station_name}_MCD15A2H_Lai_500m_*\")\n",
    "    qc_file = glob.glob(f\"{modis_path}/{station_name}_MCD15A2H_FparLai_QC*\")\n",
    "    sd_file = glob.glob(f\"{modis_path}/{station_name}_MCD15A2H_LaiStdDev_500m_*\")\n",
    "    \n",
    "    lai = pd.read_csv(lai_file[0])\n",
    "    qc = pd.read_csv(qc_file[0])\n",
    "    sd = pd.read_csv(sd_file[0])\n",
    "    # Get the number of timesteps\n",
    "    no_tsteps = min(len(lai), len(sd), len(qc)) // max(lai['pixel'])\n",
    "    \n",
    "    # Extracting pixels in the centre and immediately around it\n",
    "    pixel_no = [7, 8, 9, 12, 13, 14, 17, 18, 19]\n",
    "    \n",
    "    # Extract 3x3 pixels\n",
    "    lai_pixel = np.full((no_tsteps, len(pixel_no)), np.nan)\n",
    "    sd_pixel = np.full((no_tsteps, len(pixel_no)), np.nan)\n",
    "    qc_pixel = np.full((no_tsteps, len(pixel_no)), np.nan)\n",
    "    # Save time stamps\n",
    "    lai_time = pd.to_datetime(lai.loc[lai['pixel'] == pixel_no[0], 'calendar_date'])\n",
    "    \n",
    "    # Loop through pixels\n",
    "    for idx, p in enumerate(pixel_no):\n",
    "        #Get time series for pixel and scale using scale factor\n",
    "        lai_pixel[:, idx] = lai.loc[lai['pixel'] == p, 'value'].values[:no_tsteps] * lai.loc[lai['pixel'] == p, 'scale'].values[0]\n",
    "        sd_pixel[:, idx] = sd.loc[sd['pixel'] == p, 'value'].values[:no_tsteps] * sd.loc[sd['pixel'] == p, 'scale'].values[0]\n",
    "        qc_pixel[:, idx] = qc.loc[qc['pixel'] == p, 'value'].values[:no_tsteps]\n",
    "    \n",
    "    #print(\"Spatial Weighing started\")\n",
    "    weighted_lai_values = get_spatial_weighted_LAI(lai_pixel,sd_pixel,qc_pixel)\n",
    "    #print(\"Interpolating NAs\")\n",
    "    filled_lai = interpolate_NA_LAI(weighted_lai_values)\n",
    "    #print(\"checking data availability\")\n",
    "    gap_free_lai, selected_dates = check_data_availability_LAI(filled_lai,lai_time)\n",
    "    #print(\"Smoothing LAI\")\n",
    "    smooth_lai = smoothing_LAI(gap_free_lai)\n",
    "    \n",
    "    df_lai = pd.DataFrame({'Date':selected_dates, 'LAI' :smooth_lai})\n",
    "    \n",
    "    #Setting Date column as index to do resample to 30 mins, which is the resolution of flux tower.\n",
    "    df_lai.set_index('Date', inplace=True)\n",
    "    df_filled = df_lai.resample('30min').ffill() #This does not have date beyond 2020-12-26. \n",
    "    \n",
    "    # Reindex to extend the index until '2020-12-31' and forward fill\n",
    "    end_date_extend = pd.to_datetime('2023-12-31 23:30:00')\n",
    "    df_filled = df_filled.reindex(pd.date_range(start=df_filled.index.min(), end=end_date_extend, freq='30min')).ffill()\n",
    "    \n",
    "    filtered_df = df_filled[(df_filled.index >= start_date) & (df_filled.index <= end_date)]\n",
    "    lai_array = filtered_df['LAI'].values\n",
    "    \n",
    "    return lai_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692a331c-21ef-4e58-871e-63cbdef9a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are to list the csv files of the ICOS stations. \n",
    "# Define paths and parameters\n",
    "modis_path = \"/home/khanalp/task1/data/MODIS_Raw/MODIS_Raw\"\n",
    "cams_path = \"/home/khanalp/task1/data/cams/cams_europe.nc\"\n",
    "ICOS_location = \"/home/khanalp/task1/data/\"\n",
    "output_directory = '/home/khanalp/task1/data/ICOS/Input_data'\n",
    "prefix = \"FLX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ea5528d-35dd-47f2-b421-641823802d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting required variables from ICOS data for input.\n",
    "variables = [\n",
    "    'TIMESTAMP_START',\n",
    "    'TA_F',\n",
    "    'TA_F_QC',\n",
    "    'SW_IN_F',\n",
    "    'SW_IN_F_QC',\n",
    "    'LW_IN_F',\n",
    "    'LW_IN_F_QC',\n",
    "    'VPD_F',\n",
    "    'VPD_F_QC',\n",
    "    'PA_F',\n",
    "    'PA_F_QC',\n",
    "    'P_F',\n",
    "    'P_F_QC',\n",
    "    'WS_F',\n",
    "    'WS_F_QC',\n",
    "    'RH',\n",
    "    'CO2_F_MDS',\n",
    "    'CO2_F_MDS_QC' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "926e20f7-3860-4c74-a0d8-a90603876ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming them \n",
    "rename = {'TA_F':'Tair',\n",
    "          'TA_F_QC':'Tair_qc',\n",
    "          'SW_IN_F':'SWdown',\n",
    "          'SW_IN_F_QC':'SWdown_qc',\n",
    "          'LW_IN_F':'LWdown',\n",
    "          'LW_IN_F_QC':'LWdown_qc',\n",
    "          'VPD_F':'VPD',\n",
    "          'VPD_F_QC':'VPD_qc',\n",
    "          'PA_F':'Psurf',\n",
    "          'PA_F_QC':'Psurf_qc',\n",
    "          'P_F' : 'Precip',\n",
    "          'P_F_QC':'Precip_qc',\n",
    "          'WS_F':'Wind',\n",
    "          'WS_F_QC':'Wind_qc',\n",
    "          'CO2_F_MDS':'CO2air',\n",
    "          'CO2_F_MDS_QC':'CO2air_qc'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac3206d5-8265-4334-a8c1-59a4b3f0ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = list_folders_with_prefix(ICOS_location, prefix)\n",
    "\n",
    "csv_files_with_keyword = []\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(ICOS_location, folder)\n",
    "    csv_files_with_keyword.extend(list_csv_files_in_folder(folder_path, \"FULLSET_HH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21e4e1b5-aa09-4a84-ba9c-cea0340eb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains all the stations of the ICOS with information like plant canopy height, measurement height, etc. \n",
    "station_all = pd.read_excel(\"station_with_elevation_heightcanopy.xlsx\")\n",
    "# Set the \"station_name\" column as the index\n",
    "station_all = station_all.set_index('station_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f10a41-0af3-4f7d-8f96-8bd6d84bf05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_stations contains the stations with no NA values\"\n",
    "selected_station = pd.read_csv(\"sites_with_noNAs.csv\", index_col= 0)\n",
    "\n",
    "# These are station for which we will drive the model. \n",
    "station = station_all[station_all.index.isin(selected_station.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f580953-2213-4e28-a5c2-10f76a80093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store station_name, latitude, and longitude and other information..\n",
    "station_dict = {}\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in station.iterrows():\n",
    "    # Extract the station_name and position\n",
    "    station_name = index\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    elevation = row['elevation']\n",
    "    IGBP_longname = row['IGBP long name']\n",
    "    IGBP_shortname = row['IGBP short name']\n",
    "    #start_year = row['Start_Year_Threshold']\n",
    "    #end_year = row['End_Year_Threshold']\n",
    "    \n",
    "    # Extract height_canopy from 'height_canopy_field_information' column or 'height_canopy_ETH' column\n",
    "    height_canopy = row['height_canopy_field_information']\n",
    "    if pd.isna(height_canopy):  # Check if height_canopy_field_information is NaN\n",
    "        height_canopy = row['height_canopy_ETH']\n",
    "\n",
    "    measurement_height = row['Measurement height']\n",
    "    \n",
    "    # Store the station_name, latitude, and longitude in the dictionary\n",
    "    station_dict[station_name] = {\n",
    "        'latitude': latitude, \n",
    "        'longitude': longitude, \n",
    "        'elevation' : elevation,\n",
    "        'IGBP_longname': IGBP_longname,\n",
    "        'IGBP_shortname': IGBP_shortname,\n",
    "        'height_canopy': height_canopy,\n",
    "        'measurement_height': measurement_height,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28545682-8dab-4b7e-a25c-4a211ca26207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE-Lon_started\n",
      "BE-Lon_completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1080667/3627373940.py:36: RuntimeWarning: Mean of empty slice\n",
      "  weighted_lai = np.nanmean(weighted_lai_values, axis=1)\n",
      "/home/khanalp/anaconda3/lib/python3.11/site-packages/xarray/core/accessor_dt.py:72: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  values_as_series = pd.Series(values.ravel(), copy=False)\n",
      "/home/khanalp/anaconda3/lib/python3.11/site-packages/xarray/core/accessor_dt.py:72: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  values_as_series = pd.Series(values.ravel(), copy=False)\n"
     ]
    }
   ],
   "source": [
    "for station_name, station_info in station_dict.items():\n",
    "    print(f\"{station_name}_started\")\n",
    "    for item in csv_files_with_keyword:\n",
    "        if station_name in item:\n",
    "            selected_item = item\n",
    "            break\n",
    "            \n",
    "    # read ICOS data for station_name. \n",
    "    df = pd.read_csv(selected_item)\n",
    "\n",
    "    # Make xraay dataaset\n",
    "    xds = xr.Dataset.from_dataframe(df)\n",
    "\n",
    "    # Select variables and rename them.\n",
    "    xds_renamed = xds[variables].rename(rename)\n",
    "    \n",
    "    #Making date and time as index; renaming index to time\n",
    "    xds_indexed = xds_renamed.assign_coords(index=pd.to_datetime(xds_renamed['TIMESTAMP_START'], format='%Y%m%d%H%M')).rename({'index':'time'})\n",
    "   # Adding x,y to the dimensions; also dropping variable 'TIMESTAMP_START' because its already indexed\n",
    "    xds_dimension = xds_indexed.expand_dims({'x': [1], 'y': [2]}).drop_vars('TIMESTAMP_START')\n",
    "    #Converting x,y to float64 \n",
    "    xds_dimension['x'] = xds_dimension['x'].astype('float64')\n",
    "    xds_dimension['y'] = xds_dimension['y'].astype('float64')\n",
    "\n",
    "    # Assign the coordinate arrays to the dataset\n",
    "    xds_dimension = xds_dimension.assign_coords({'x': [station_info['longitude']]  , 'y': [station_info['latitude']]})\n",
    "    # Convert the desired date to a pandas Timestamp\n",
    "    \n",
    "    # Get the minimum time value in the dataset\n",
    "    min_time = pd.Timestamp(xds_dimension.time.min().values)\n",
    "\n",
    "    # Set the start_date to 2012-01-01 if the minimum time is before 2012, otherwise keep it the same\n",
    "    start_date = pd.Timestamp('2012-01-01 00:00:00') if min_time < pd.Timestamp('2012-01-01') else min_time\n",
    "    \n",
    "    # Select data from 'xds_dimension' after the start_date\n",
    "    xds_dimension = xds_dimension.sel(time=slice(start_date, None))\n",
    "\n",
    "    counts = {var: np.count_nonzero(xds_dimension[var].values == -9999) for var in xds_dimension.data_vars}\n",
    "\n",
    "    if counts['RH'] > 0: \n",
    "        # #Grabbing input vpd,Tair, pressure\n",
    "        vpd = xds_dimension.VPD.values.flatten()\n",
    "        Tair = xds_dimension.Tair.values.flatten()\n",
    "        pressure = xds_dimension .Psurf.values.flatten()\n",
    "\n",
    "        # calculating RH and q \n",
    "        RH, q = calculate_RH_and_q(vpd, Tair, pressure)\n",
    "        \n",
    "        xds_dimension['Qair'] = xr.DataArray(q.reshape(1,1,-1), dims=['x','y','time'])\n",
    "        xds_dimension['RH'] = xr.DataArray(RH.reshape(1,1,-1), dims=['x','y','time'])\n",
    "\n",
    "    if counts['CO2air'] > 0:\n",
    "        #getting cams data \n",
    "        co2 = get_cams_co2_data(xds_dimension['x'], xds_dimension['y'], xds_dimension.time.min().values, xds_dimension.time.max().values, cams_path)\n",
    "        xds_dimension['CO2air'] = xr.DataArray(co2.reshape(1,1,-1), dims=['x','y','time'])\n",
    "        \n",
    "    # getting LAI    \n",
    "    lai = get_LAI(modis_path,station_name,xds_dimension.time.values.min(),xds_dimension.time.values.max())\n",
    "    xds_dimension['LAI'] = xr.DataArray(lai.reshape(1,1,-1), dims=['x','y','time'])\n",
    "    xds_dimension['LAI_alternative'] = xr.DataArray(lai.reshape(1,1,-1), dims=['x','y','time'])\n",
    "\n",
    "    #getting remaining variables in xarray.\n",
    "    xds_dimension['latitude'] = xr.DataArray(np.array(station_info['latitude']).reshape(1,-1), dims=['x','y'])\n",
    "    xds_dimension['longitude'] = xr.DataArray(np.array(station_info['longitude']).reshape(1,-1), dims=['x','y'])\n",
    "    xds_dimension['reference_height'] = xr.DataArray(np.array(station_info['measurement_height']).reshape(1,-1), dims=['x','y'])\n",
    "    xds_dimension['canopy_height'] = xr.DataArray(np.array(station_info['height_canopy']).reshape(1,-1), dims=['x','y'])\n",
    "    xds_dimension['elevation'] = xr.DataArray(np.array(station_info['elevation']).reshape(1,-1), dims=['x','y'])\n",
    "    xds_dimension['IGBP_veg_short'] = xr.DataArray(np.array(station_info['IGBP_shortname'], dtype = 'S200'))\n",
    "    xds_dimension['IGBP_veg_long'] = xr.DataArray(np.array(station_info['IGBP_longname'], dtype = 'S200'))\n",
    "    \n",
    "    for var_name in xds_dimension.data_vars:\n",
    "        if var_name not in ['IGBP_veg_short', 'IGBP_veg_long']:\n",
    "            xds_dimension[var_name] = xds_dimension[var_name].astype('float32')\n",
    "    # Changing the units\n",
    "    xds_dimension['Precip'] = xds_dimension['Precip'] / (30*60) # changing from mm of 30 mins to mm/s. \n",
    "    xds_dimension['Tair'] = xds_dimension['Tair'] + 273.15 # degree C to kelvin\n",
    "    xds_dimension['Psurf'] = xds_dimension['Psurf']*1000 #kPa to Pa\n",
    "    #Saving file\n",
    "    min_year = xds_dimension.time.dt.year.min().item()\n",
    "    max_year = xds_dimension.time.dt.year.max().item()\n",
    "    file_name = f\"{station_name}_{min_year}-{max_year}_FLUXNET2015_Met.nc\"\n",
    "    xds_dimension.to_netcdf(os.path.join(output_directory, file_name))\n",
    "    print(f\"{station_name}_completed\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cf9c1-4ffe-40e0-97ae-c73c6b34cfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc1fe36ed0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xds_dimension.Psurf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534a487-6960-432f-a69c-8df9d556c31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
