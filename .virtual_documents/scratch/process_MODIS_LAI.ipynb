


import os
import numpy as np
import pandas as pd
import xarray as xr
from scipy.interpolate import CubicSpline
import matplotlib.pyplot as plt


# This contains all the tif files for the certain location.
file_path = "/home/khanalp/task1/data/MODIS/MODIS_LAI_Export_3"


# Get a list of all .tif files in the directory
tif_files = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.tif')]


# The empty weighted_means calculate the spatial weighted mean; steps (1-4) in the above things to do'
weighted_means = []
# List to store the dates
dates = []
# Define the location
#This is the location of the flux sites, later this should be read from the folder.
target_location = (2.284102, 47.322918) 
# The MODIS data comes with the qc flag, these are only the flags which have good quality data. 
# Reference from the Ukkola, PLUMBER2 paper.
qc_allowed = [0, 2, 24, 26, 32, 34, 56, 58]


for file in sorted(tif_files):
    #sorted to sort it by alphabetical order. 

    ## Grabbing date from the folder name
    # Split the file path by '/'
    parts = file.split('/')
    
    # Get the filename part #this part contains date. 
    filename = parts[-1] 
    
    # Extract the date part
    date_str = filename[9:17]  # Adjusted to match the date format in the example
    
    # Convert date string to np.datetime64 format
    date = np.datetime64(f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}")
    
    # Append the date to the list
    dates.append(date)
    ## Grabbing date from the folder name

    # open as xarray
    da = xr.open_dataarray(file)

    # Getting the nearest surrounding eight grid cells from the tif files. 

    # Calculate the distances to all points
    distances = np.sqrt((da.x - target_location[0])**2 + (da.y - target_location[1])**2).values.transpose()
    nearest_idx = np.unravel_index(np.argmin(distances), distances.shape)
    
    # Define the window size (assuming 3x3 grid around the nearest cell)
    window_size = 1

    # Extract the subset of the data array centered around the nearest cell
    subset_da = da.isel(
        x=slice(nearest_idx[1] - window_size, nearest_idx[1] + window_size + 1),
        y=slice(nearest_idx[0] - window_size, nearest_idx[0] + window_size + 1)
    )

    # THe LAI value should be scaled by 0.1, refer to the MODIS data user guide. 
    
    LAI = subset_da.sel(band=1).values*0.1
    qc = subset_da.sel(band=2).values
    std = subset_da.sel(band=3).values*0.1

    # Create mask allowing only qc that are present in the list qc_allowed. 
    qc_mask = np.isin(qc, qc_allowed)


    #Also mask out where sd really low (likely cloud effects)
    # Define the threshold value for replacing values with NaN 
    threshold = 0.1
    # Check if each item in the std array is equal to the threshold
    std_mask = std >= threshold
    # Combined mask
    mask = qc_mask*std_mask
    
    if np.sum(mask) > 0:
        #if no data available after masking, replacing by NA else calculating the weighted mean. 
        
        #Weight grid cell estimates by their standard deviation
          #Following Martin's method (https://github.com/mdekauwe/get_MODIS_LAI_australia/blob/master/build_modis_climatology.py),
          #but normalising by sum of standard deviations
        
        # Replace values not in qc_allowed with NA
        qc_processed = np.where(mask, qc, np.nan)
        # Replace values equal to the threshold with NaN
        std_processed = np.where(mask, std, np.nan)
        lai_processed = np.where(mask,LAI,np.nan)

        #weights are from the standard error. 
        weights = 1 / (std_processed**2)
        real_weight= weights/np.nansum(weights)
        weighted_mean = np.nansum(lai_processed * real_weight) / np.nansum(real_weight)
        weighted_means.append(weighted_mean)
    else:
        weighted_means.append(np.nan)



#Changing to numpy array
dates = np.array(dates)
len(weighted_means)





# Create a DataFrame and changing date to datetime objects
df = pd.DataFrame({'Date': dates, 'Weighted_Mean': weighted_means})
df['Date'] = pd.to_datetime(df['Date'])


# Plot interpolated values
plt.plot(df['Date'], df['Weighted_Mean'])
plt.xlabel('Date')
plt.ylabel('Spatial Weighted Mean of LAI')
plt.title('Original MODIS')
plt.legend()
plt.show()





## Here I will fill all the gaps in the 8-day time series using cubic spline function.

# Create a copy of the original DataFrame
df_filled = df.copy()

# Convert 'Date' column to numeric representation (e.g., days since the first date)
df_filled['Date_numeric'] = (df_filled['Date'] - df_filled['Date'].min()).dt.days
    #(df_filled['Date'] - df_filled['Date'].min()) this calculates the difference of the date from the min date. 
    # and .dt.days gets it in the number of days. 

# Create a mask for NaN values in 'Weighted_Mean'
mask = df_filled['Weighted_Mean'].isna()

# Interpolate NaN values using cubic spline
cs = CubicSpline(df_filled.loc[~mask, 'Date_numeric'], df_filled.loc[~mask, 'Weighted_Mean'])

interpolated_values = cs(df_filled.loc[mask, 'Date_numeric'])

# Fill NaN values with interpolated values only where they exist
df_filled.loc[mask, 'Weighted_Mean'] = interpolated_values

# Drop the 'Date_numeric' column
df_filled.drop(columns=['Date_numeric'], inplace=True)


plt.plot(df['Date'], df['Weighted_Mean'], label='Original Data')
# Find indices of NaN values in the original 'Weighted_Mean' column
nan_indices = df['Weighted_Mean'].isna()

# Plot interpolated values
plt.plot(df['Date'][nan_indices], interpolated_values, 'ro', label='Interpolated Values')

plt.xlabel('Date')
plt.ylabel('Weighted Mean')
plt.title('Interpolating the data')
plt.legend()
plt.show()



# Plot interpolated values
plt.plot(df_filled['Date'], df_filled['Weighted_Mean'])

plt.xlabel('Date')
plt.ylabel('Weighted Mean')
plt.title('Interpolated Values')
plt.legend()
plt.show()


df_filled_positive = df_filled.copy()
# Subset the DataFrame where 'Weighted_Mean' is negative and replace those values with 0
df_filled_positive.loc[df_filled_positive['Weighted_Mean'] < 0, 'Weighted_Mean'] = 0


# Plot interpolated values
plt.plot(df_filled_positive['Date'], df_filled_positive['Weighted_Mean'])

plt.xlabel('Date')
plt.ylabel('Weighted Mean')
plt.title('Interpolated Values')
plt.legend()
plt.show()


#I do not see the purpose of smoothing again as done in PLUMBER2.
# Convert 'Date' column to numeric representation (e.g., days since the first date)
df_filled_positive['Date_numeric'] = (df_filled_positive['Date'] - df_filled_positive['Date'].min()).dt.days

# Create a cubic spline interpolation function
cs = CubicSpline(df_filled_positive['Date_numeric'], df_filled_positive['Weighted_Mean'])

# Generate smoothed values by evaluating the interpolation function at the original x-values
smoothed_values = cs(df_filled_positive['Date_numeric'])

# Replace negative values with 0
smoothed_values[smoothed_values < 0] = 0

# Update 'Weighted_Mean' column with smoothed values
df_filled_positive['Smoothed_Weighted_Mean'] = smoothed_values

# Drop the 'Date_numeric' column if you don't need it anymore
df_filled_positive.drop(columns=['Date_numeric'], inplace=True)

df_filled_positive['diff'] = df_filled_positive['Weighted_Mean'] - df_filled_positive['Smoothed_Weighted_Mean']
np.sum(df_filled_positive['diff'])


#Lets count how many entries per year are in the MODIS data. 

date_counts = df_filled.groupby(df_filled['Date'].dt.year).size()

# Print the counts
print("Number of dates in each year:")
print(date_counts)


# Creating an empty dataframe date_df with dates (46 frequency) 8 days apart. 
# Initialize an empty list to store the datetime series
date_series = []

# Loop over each year from 2000 to 2020
for year in range(2002, 2024):
    # Generate the first two dates for the year
    #year_dates = pd.date_range(start=f"{year}-01-01", periods=2, freq='8D')
    
    # Ensure 46 evenly spaced dates for the rest of the year
    year_dates = pd.date_range(start=f"{year}-01-01", end=f"{year}-12-31", freq='8D')
    
    # Append the datetime series for the current year to the list
    date_series.extend(year_dates)

# Convert the list of datetime series to a pandas DataFrame
date_df = pd.DataFrame(date_series, columns=['Date'])

print(date_df)





# Initialize the new column  "LAI" in date_df with NaN values
date_df['LAI'] = np.nan


# Iterate over the rows of df_date
for index, row in date_df.iterrows():
    # Check if there is a corresponding date in df_filled_positive that matches or differs by one day
    matching_date = df_filled_positive[(df_filled_positive['Date'] == row['Date']) |
                                        (df_filled_positive['Date'] == row['Date'] + pd.Timedelta(days=1))]
    
    # If matching date(s) found, copy the value of Smoothed_Weighted_Mean to New_Column
    if not matching_date.empty:
        date_df.at[index, 'LAI'] = matching_date.iloc[0]['Smoothed_Weighted_Mean']


# Convert the 'LAI' column to a NumPy array
lai_array = date_df['LAI'].to_numpy().reshape(-1, 46)
lai_array.shape


#This calculates the climatology, seasonal mean. 
column_means = np.nanmean(lai_array, axis=0)
column_means.shape


# Subtract the column mean from each column (subtracting the climatology to get anomalies)
centered_data = lai_array - column_means


# flattened_data = centered_data.flatten()
# Plot flattened_data
plt.figure(figsize=(10, 5))
plt.plot(lai_array.flatten(), label='Original Data')
plt.plot(np.tile(column_means.flatten(),21), label='Seasonal')
plt.plot(centered_data.flatten(), label='Anomaly')
plt.xlabel('Index')
plt.ylabel('Value')
#plt.title('Anomaly Data')
plt.legend()
plt.grid(True)
plt.show()


# Assuming centered_data contains the centered data after subtracting column means
# Convert the centered data to a pandas DataFrame
df_centered = pd.DataFrame(centered_data.flatten())

# Compute the rolling mean anomaly with a window size of 12 (equivalent to +/- 6 months)
df_smoothed = df_centered.rolling(window=12, min_periods=1).mean()

#print("Rolling mean anomaly:\n", anomaly)
df_smoothed.columns= ['anomaly']


# flattened_data = centered_data.flatten()
# Plot flattened_data
plt.figure(figsize=(10, 5))
plt.plot(centered_data.flatten(), label='Raw Anomaly')
plt.plot(df_smoothed['anomaly'], label='Smooth Anomaly')
plt.xlabel('Index')
plt.ylabel('Value')
#plt.title('Anomaly Data')
plt.legend()
plt.grid(True)
plt.show()


# Repeat column_means 46 times
repeated_column_means = np.tile(column_means, reps=21)


# Create a new column in df with the repeated column_means
df_smoothed['Seasonal'] = repeated_column_means.flatten()
# Replace NA values in Column_Means with NA where anomaly column has NA
df_smoothed['Seasonal'][df_smoothed['anomaly'].isna()] = np.nan

#To get final processed LAI, we add seasonal back to smooth anomaly.
df_smoothed['LAI_final']= df_smoothed['Seasonal']+df_smoothed['anomaly']





#flattened_data = centered_data.flatten()
# Plot flattened_data
plt.figure(figsize=(10, 5))
plt.plot(lai_array.flatten(), label='Original Data')
#plt.plot(df['anomaly'], label='Anomaly')
#plt.plot(df['Seasonal'], label='Seasonal')
plt.plot(df_smoothed['LAI_final'], label='Smoothed LAI')
plt.xlabel('Index (Date)')
plt.ylabel('LAI')
#plt.title('Anomaly Data')
plt.legend()
plt.grid(True)
plt.show()


# Adding date back to df_smoothed
df_smoothed['Date'] = date_series
df_selected = df_smoothed[['Date','LAI_final']]


#Setting Date column as index to do resample to 30 mins, which is the resolution of flux tower.
df_selected.set_index('Date', inplace=True)
df_filled = df_selected.resample('30min').ffill()
#This does not have date beyond 2020-12-26. 


# Reindex to extend the index until '2020-12-31' and forward fill
end_date_extend = pd.to_datetime('2020-12-31')
df_filled = df_filled.reindex(pd.date_range(start=df_filled.index.min(), end=end_date_extend, freq='30min')).ffill()

df_filled.head()


#flattened_data = centered_data.flatten()
# Plot flattened_data
plt.figure(figsize=(10, 5))
#plt.plot(lai_array_reshaped.flatten(), label='Original Data')
#plt.plot(df['anomaly'], label='Anomaly')
#plt.plot(df['Seasonal'], label='Seasonal')
plt.plot(df_filled['LAI_final'], label='Final')
plt.xlabel('Index')
plt.ylabel('LAI')
#plt.title('Anomaly Data')
plt.legend()
plt.grid(True)
plt.show()





# Take this input from other files later. 
flux_start = "2017-01-01"
flux_end = "2020-12-31"

df_LAI = df_filled['2017-01-01':'2020-12-31']


# Save DataFrame as an Excel file
df_LAI.to_excel('/home/khanalp/task1/output/data_LAI_processed.xlsx', index=True) 


abc = pd.read_excel('/home/khanalp/task1/output/data_LAI_processed.xlsx', index_col = 0)


#flattened_data = centered_data.flatten()
# Plot flattened_data
plt.figure(figsize=(10, 5))
#plt.plot(lai_array_reshaped.flatten(), label='Original Data')
#plt.plot(df['anomaly'], label='Anomaly')
#plt.plot(df['Seasonal'], label='Seasonal')
plt.plot(df_LAI['LAI_final'], label='Final')
plt.xlabel('Date')
plt.ylabel('LAI')
#plt.title('Anomaly Data')
plt.legend()
plt.grid(True)
plt.show()



