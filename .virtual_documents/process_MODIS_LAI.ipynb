


import os 
import xarray as xr
import numpy as np
from PIL import Image
import pandas as pd
from scipy.interpolate import CubicSpline


file_path = "/home/khanalp/task1/data/MODIS/MODIS_LAI_Export_3"


# Get a list of all .tif files in the directory
tif_files = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.endswith('.tif')]


# Read each .tif file into a numpy array and store them in a list
arrays = []
for tif_file in tif_files:
    with Image.open(tif_file) as img:
        arrays.append(np.array(img))

# If you want to stack the arrays into a single array, assuming they all have the same shape
stacked_array = np.stack(arrays)


stacked_array.shape


# Subset the array to take only the first place along the last dimension
Lai_500m = stacked_array[..., 0]
FparLai_QC = stacked_array[..., 1]
LaiStdDev_500m = stacked_array[..., 2]


Lai_500m_transposed = np.transpose(Lai_500m, axes=(2, 1, 0))
FparLai_QC_transposed = np.transpose(FparLai_QC, axes=(2, 1, 0))
LaiStdDev_500m_transposed = np.transpose(LaiStdDev_500m, axes=(2, 1, 0))


FparLai_QC[1,...]


tif_files[0]


# Open each .tif file into a separate xarray dataset
da = xr.open_dataarray(tif_files[0])



weighted_means = []
# List to store the dates
dates = []

# Define the location
target_location = (2.284102, 47.322918)
qc_allowed = [0, 2, 24, 26, 32, 34, 56, 58]

for file in sorted(tif_files):
    # Split the file path by '/'
    parts = file.split('/')
    
    # Get the filename part
    filename = parts[-1]
    
    # Extract the date part
    date_str = filename[9:17]  # Adjusted to match the date format in the example
    
    # Convert date string to np.datetime64 format
    date = np.datetime64(f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}")
    
    # Append the date to the list
    dates.append(date)
    da = xr.open_dataarray(file)
    # Calculate the distances to all points
    distances = np.sqrt((da.x - target_location[0])**2 + (da.y - target_location[1])**2).values.transpose()
    nearest_idx = np.unravel_index(np.argmin(distances), distances.shape)
    # Define the window size (assuming 3x3 grid around the nearest cell)
    window_size = 1

    # Extract the subset of the data array centered around the nearest cell
    subset_da = da.isel(
        x=slice(nearest_idx[1] - window_size, nearest_idx[1] + window_size + 1),
        y=slice(nearest_idx[0] - window_size, nearest_idx[0] + window_size + 1)
    )
    LAI = subset_da.sel(band=1).values*0.1
    qc = subset_da.sel(band=2).values
    std = subset_da.sel(band=3).values*0.1
    qc_mask = np.isin(qc, qc_allowed)
    # Define the threshold value for replacing values with NaN
    threshold = 0.1

# Check if each item in the std array is equal to the threshold
    std_mask = std >= threshold

    mask = qc_mask*std_mask
    if np.sum(mask) > 0:
        # Replace values not in qc_allowed with NA
        qc_processed = np.where(mask, qc, np.nan)
        # Replace values equal to the threshold with NaN
        std_processed = np.where(mask, std, np.nan)
        lai_processed = np.where(mask,LAI,np.nan)
        weights = 1 / (std_processed**2)
        real_weight= weights/np.nansum(weights)
        weighted_mean = np.nansum(lai_processed * real_weight) / np.nansum(real_weight)
        weighted_means.append(weighted_mean)
    else:
        weighted_means.append(np.nan)



dates = np.array(dates)


# Create a DataFrame
df = pd.DataFrame({'Date': dates, 'Weighted_Mean': weighted_means})
df


# Optionally, if dates are supposed to be datetime objects, you can convert them
df['Date'] = pd.to_datetime(df['Date'])
#df['Date'].tolist()


# Create a copy of the original DataFrame
df_filled = df.copy()

# Convert 'Date' column to numeric representation (e.g., days since the first date)
df_filled['Date_numeric'] = (df_filled['Date'] - df_filled['Date'].min()).dt.days

# Create a mask for NaN values in 'Weighted_Mean'
mask = df_filled['Weighted_Mean'].isna()

# Interpolate NaN values using cubic spline
cs = CubicSpline(df_filled.loc[~mask, 'Date_numeric'], df_filled.loc[~mask, 'Weighted_Mean'])
interpolated_values = cs(df_filled.loc[mask, 'Date_numeric'])


import matplotlib.pyplot as plt

# Plot original data
plt.plot(df['Date'], df['Weighted_Mean'], label='Original Data')
# Find indices of NaN values in the original 'Weighted_Mean' column
nan_indices = df['Weighted_Mean'].isna()

# Plot interpolated values
plt.plot(df['Date'][nan_indices], interpolated_values, 'ro', label='Interpolated Values')

plt.xlabel('Date')
plt.ylabel('Weighted Mean')
plt.title('Interpolated Values')
plt.legend()
plt.show()



# Fill NaN values with interpolated values only where they exist
df_filled.loc[mask, 'Weighted_Mean'] = interpolated_values

# Drop the 'Date_numeric' column
df_filled.drop(columns=['Date_numeric'], inplace=True)


# Plot interpolated values
plt.plot(df_filled['Date'], df_filled['Weighted_Mean'])

plt.xlabel('Date')
plt.ylabel('Weighted Mean')
plt.title('Interpolated Values')
plt.legend()
plt.show()


df_filled_positive = df_filled.copy()
# Subset the DataFrame where 'Weighted_Mean' is negative and replace those values with 0
df_filled_positive.loc[df_filled_positive['Weighted_Mean'] < 0, 'Weighted_Mean'] = 0


#I do not see the purpose of smoothing again as done in PLUMBER2.
# Convert 'Date' column to numeric representation (e.g., days since the first date)
df_filled_positive['Date_numeric'] = (df_filled_positive['Date'] - df_filled_positive['Date'].min()).dt.days

# Create a cubic spline interpolation function
cs = CubicSpline(df_filled_positive['Date_numeric'], df_filled_positive['Weighted_Mean'])

# Generate smoothed values by evaluating the interpolation function at the original x-values
smoothed_values = cs(df_filled_positive['Date_numeric'])

# Replace negative values with 0
smoothed_values[smoothed_values < 0] = 0

# Update 'Weighted_Mean' column with smoothed values
df_filled_positive['Smoothed_Weighted_Mean'] = smoothed_values

# Drop the 'Date_numeric' column if you don't need it anymore
df_filled_positive.drop(columns=['Date_numeric'], inplace=True)

df_filled_positive['diff'] = df_filled_positive['Weighted_Mean'] - df_filled_positive['Smoothed_Weighted_Mean']


# Plot interpolated values
plt.plot(df_filled_positive['Date'], df_filled_positive['Smoothed_Weighted_Mean'])

plt.xlabel('Date')
plt.ylabel('Weighted Mean')
plt.title('Interpolated Values')
plt.legend()
plt.show()


weighted_means_interpolation.shape


weighted_means_interpolation[weighted_means_interpolation < 0] = 0


# Flatten the array
weighted_means_interpolation = weighted_means_interpolation.flatten()


import matplotlib.pyplot as plt

# Create x-axis values (e.g., indices of the list)
x_values = range(len(weighted_means_interpolation[0:94]))

# Plot the weighted_means
plt.plot(x_values, weighted_means_interpolation[0:94], marker='o', linestyle='-')

# Add labels and title
plt.xlabel('File Index')
plt.ylabel('Weighted Mean')
plt.title('Weighted Means for Each File')

# Show the plot
plt.grid(True)
plt.tight_layout()
plt.show()


# List to store the dates
dates = []

# Iterate over each file path in tif_files
for file_path in tif_files:
    # Split the file path by '/'
    parts = file_path.split('/')
    
    # Get the filename part
    filename = parts[-1]
    
    # Extract the date part
    date_str = filename[9:17]  # Adjusted to match the date format in the example
    
    # Convert date string to np.datetime64 format
    date = np.datetime64(f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}")
    
    # Append the date to the list
    dates.append(date)

# Convert the list of dates to np.array
dates = np.array(dates)
len(dates)


file_path = tif_files[0]

# Split the file path by '/'
parts = file_path.split('/')

# Get the filename part
filename = parts[-1]
print(filename)
# Extract the date part
date_str = filename[9:18]

# Convert date string to the desired format
date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"


# Converting to DataFrame
df = band_1.to_dataframe()


# Resetting index to get 'y' as row names
df.reset_index(inplace=True)


# Renaming columns
df = df[['y','x','band_data']]


# Reshaping the DataFrame to have 'x' as columns and 'y' as rows
df_pivoted = df.pivot(index='y', columns='x', values='band_data')


df_pivoted


da


# Define the location
target_location = (2.284102, 47.322918)


# Calculate the distances to all points
distances = np.sqrt((da.x - target_location[0])**2 + (da.y - target_location[1])**2).values.transpose()


nearest_idx = np.unravel_index(np.argmin(distances), distances.shape)
nearest_idx 


window_size =1
print(nearest_idx[1] - window_size, nearest_idx[1] + window_size + 1)
print(nearest_idx[0] - window_size, nearest_idx[0] + window_size + 1)


# Define the window size (assuming 3x3 grid around the nearest cell)
window_size = 1

# Extract the subset of the data array centered around the nearest cell
subset_da = da.isel(
    x=slice(nearest_idx[1] - window_size, nearest_idx[1] + window_size + 1),
    y=slice(nearest_idx[0] - window_size, nearest_idx[0] + window_size + 1)
)





LAI = subset_da.sel(band=1).values*0.1
qc = subset_da.sel(band=2).values
std = subset_da.sel(band=3).values*0.1


qc


qc_allowed = [0, 2, 24, 26, 32, 34, 56, 58]


# Check if each item in the qc array is in qc_allowed
qc_mask = np.isin(qc, qc_allowed)

# Replace values not in qc_allowed with NA
qc_processed = np.where(mask, qc, np.nan)


# Define the threshold value for replacing values with NaN
threshold = 0.1

# Check if each item in the std array is equal to the threshold
std_mask = std > threshold

# Replace values equal to the threshold with NaN
std_processed = np.where(std_mask, np.nan, std)


qc_mask


std_mask


mask = qc_mask*std_mask
mask


np.sum(mask)


# Replace values not in qc_allowed with NA
qc_processed = np.where(mask, qc, np.nan)
# Replace values equal to the threshold with NaN
std_processed = np.where(mask, std, np.nan)
lai_processed = np.where(mask,LAI,np.nan)


lai_processed


# Convert NaN values to 0
weights = 1 / (std_processed**2)


weights


weights_sum = np.nansum(weights)


real_weight= weights/np.nansum(weights)
real_weight
np.nansum(real_weight)


lai_processed * real_weight


weighted_mean = np.nansum(lai_processed * real_weight) / np.nansum(real_weight)


weighted_mean


# Compute weights
weights = (1 / std_no_nan**2) / np.sum(1 / std_no_nan**2)
weights


# Compute the sum of weights, ignoring NaN values
sum_weights = np.sum(weights)

# Compute the weighted mean
weighted_mean = np.sum(lai * weights) / sum_weights


# Compute weights
weights = (1 / std_processed**2) / np.sum(1 / std_processed**2)


weights


# Convert the subset to a pandas DataFrame
df_1 = subset_da.sel(band=2).to_dataframe(name='band_value').reset_index()
df_1


# Resetting index to get 'y' as row names
df_1.reset_index(inplace=True)


# Renaming columns
df_1 = df_1[['y','x','band_value']]
df_1


# Reshaping the DataFrame to have 'x' as columns and 'y' as rows
df_pivoted_1 = df_1.pivot(index='y', columns='x', values='band_value')


df_pivoted_1






